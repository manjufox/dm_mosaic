{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from einops.layers.torch import Rearrange,Reduce\n",
    "from torchvision.io import read_image\n",
    "import torchvision\n",
    "import itertools\n",
    "from torchmetrics.image import ErrorRelativeGlobalDimensionlessSynthesis\n",
    "from torchvision.utils import make_grid, save_image\n",
    "from torchvision.transforms import v2\n",
    "import math\n",
    "import numpy as np\n",
    "import icecream\n",
    "from timm.models.vision_transformer import PatchEmbed, Attention, Mlp\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\".\")\n",
    "\n",
    "from util.debug_print import dprint\n",
    "from setting.setting import *\n",
    "import util.my_transform as mt\n",
    "\n",
    "debug=True\n",
    "\n",
    "\n",
    "class TimestepEmbedder(nn.Module):\n",
    "    \"\"\"\n",
    "    Embeds scalar timesteps into vector representations.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, frequency_embedding_size=256):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(frequency_embedding_size, hidden_size, bias=True),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden_size, hidden_size, bias=True),\n",
    "        )\n",
    "        self.frequency_embedding_size = frequency_embedding_size\n",
    "\n",
    "    @staticmethod\n",
    "    def timestep_embedding(t, dim, max_period=10000):\n",
    "        \"\"\"\n",
    "        Create sinusoidal timestep embeddings.\n",
    "        :param t: a 1-D Tensor of N indices, one per batch element.\n",
    "                          These may be fractional.\n",
    "        :param dim: the dimension of the output.\n",
    "        :param max_period: controls the minimum frequency of the embeddings.\n",
    "        :return: an (N, D) Tensor of positional embeddings.\n",
    "        \"\"\"\n",
    "        # https://github.com/openai/glide-text2im/blob/main/glide_text2im/nn.py\n",
    "        half = dim // 2\n",
    "        freqs = torch.exp(\n",
    "            -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\n",
    "        ).to(device=DEVICE)\n",
    "        args = t[:, None].float() * freqs[None]\n",
    "        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
    "        if dim % 2:\n",
    "            embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
    "        return embedding\n",
    "\n",
    "    def forward(self, t):\n",
    "        t_freq = self.timestep_embedding(t, self.frequency_embedding_size)\n",
    "        t_emb = self.mlp(t_freq)\n",
    "        return t_emb\n",
    "    \n",
    "class Conv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        super(Conv2d,self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "    def diversity_loss(self):\n",
    "        weight = self.conv.weight.view(self.conv.out_channels, -1)  # (out_channels, in_channels * kernel_size * kernel_size)\n",
    "        weight_mean = torch.mean(weight, dim=1, keepdim=True)  # (out_channels, 1)\n",
    "        weight_centered = weight - weight_mean  # (out_channels, in_channels * kernel_size * kernel_size)\n",
    "        covariance_matrix = torch.matmul(weight_centered, weight_centered.t())  # (out_channels, out_channels)\n",
    "        diag = torch.diag(covariance_matrix)\n",
    "        covariance_matrix = covariance_matrix - torch.diag_embed(diag)\n",
    "        diversity_loss = torch.mean(torch.abs(covariance_matrix))  # Sum of absolute values of off-diagonal elements\n",
    "        return diversity_loss\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,kernel_size=3,stride=2,padding=1):\n",
    "        super().__init__()\n",
    "        self.Block = nn.Sequential(\n",
    "            Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.Block(x)\n",
    "        return x\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels,kernel_size=3,stride=2,padding=1):\n",
    "        super().__init__()\n",
    "        self.Block = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.SiLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.Block(x)\n",
    "    \n",
    "class AffinDiffusion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.batchnorm = nn.BatchNorm2d(C1)\n",
    "        self.DownBlock = nn.Sequential(\n",
    "            DownBlock(C1, C2,kernel_size=4,stride=4,padding=0),\n",
    "            DownBlock(C2, C3,kernel_size=4,stride=4,padding=0),\n",
    "            DownBlock(C3, C4,kernel_size=4,stride=4,padding=0),\n",
    "            DownBlock(C4, C4,kernel_size=2,stride=2,padding=0),\n",
    "        )\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=C4*2*2, nhead=8,batch_first=True)\n",
    "        self.transformerencoder = nn.TransformerEncoder(encoder_layer, num_layers=3)\n",
    "        # self.reduce = Reduce(\"b c h w -> b c\",reduction=\"mean\")\n",
    "        self.rearrange = Rearrange(\"b c h w -> b (c h w)\")\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(C4*2*2,DENOISE_PARAMS,bias=True),\n",
    "            # nn.ReLU(),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        # self.mlp2_latent = nn.Sequential(\n",
    "        #     nn.Linear(DENOISE_PARAMS,C5),\n",
    "        #     nn.SiLU(),\n",
    "        # )\n",
    "\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=C4*2*2, nhead=8,batch_first=True)\n",
    "        self.transformerdecoder = nn.TransformerDecoder(decoder_layer, num_layers=3)\n",
    "\n",
    "        self.rearrange2 = Rearrange(\"b (c h w) -> b c h w\",c=C4,h=2,w=2)\n",
    "        \n",
    "        self.UpBlock = nn.Sequential(\n",
    "            UpBlock(C4, C4,kernel_size=2,stride=2,padding=0),\n",
    "            UpBlock(C4, C4,kernel_size=2,stride=2,padding=0),\n",
    "            UpBlock(C4, C3,kernel_size=2,stride=2,padding=0),\n",
    "            UpBlock(C3, C3,kernel_size=2,stride=2,padding=0),\n",
    "            UpBlock(C3, C2,kernel_size=2,stride=2,padding=0),\n",
    "            UpBlock(C2, C2,kernel_size=2,stride=2,padding=0),\n",
    "            UpBlock(C2, C1,kernel_size=2,stride=2,padding=0),\n",
    "        )\n",
    "\n",
    "        self.mlp2 = nn.Sequential(\n",
    "            nn.Linear(2,C4*2*2),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "\n",
    "        # self.Embedding = nn.Parameter(torch.zeros(B,C,H,W),requires_grad=True)\n",
    "        self.t = TimestepEmbedder(hidden_size=C4*2*2)\n",
    "        # self.Embedding2 = nn.Parameter(torch.zeros(B,C4*2*2),requires_grad=True)\n",
    "        self.Embedding3 = nn.Parameter(torch.zeros(B,C4,2,2),requires_grad=True)\n",
    "        self.Embedding4 = nn.Parameter(torch.zeros(B,C4,2,2),requires_grad=True)\n",
    "        # self.Embedding = nn.Embedding(B,C5)\n",
    "        # self.Embedding.weight.data = torch.randn(B,C5)\n",
    "        # self.embedding =\n",
    "        # self.Embedding = nn.Parameter(torch.randn(B,C,H,W))\n",
    "        # self.Transformer = nn.Transformer(d_model=C4, nhead=16, num_encoder_layers=12,batch_first=True)\n",
    "        # self.reduce = Reduce(\"b c h w -> b c\",reduction=\"mean\")\n",
    "        # self.xy = nn.Sequential(\n",
    "        #     nn.Linear(DENOISE_PARAMS,2),\n",
    "            # nn.Sigmoid(),\n",
    "        # )\n",
    "        \n",
    "        # self.apply(self.weight_init)\n",
    "\n",
    "    def weight_init(self,module):\n",
    "        self.mlp.weight.data = torch.zeros(DENOISE_PARAMS,C4) + 0.001\n",
    "        self.mlp.bias.data = torch.zeros(DENOISE_PARAMS) + 0.001\n",
    "\n",
    "    #     if isinstance(module, nn.ConvTranspose2d):\n",
    "    #         module.weight.data = torch.zeros_like(module.weight.data) + 0.001\n",
    "            # if module.bias is not None:\n",
    "            #     module.bias.data = torch.zeros_like(module.bias.data) + 0.001\n",
    "\n",
    "    def forward(self, x,i=torch.tensor([100],device=DEVICE)):\n",
    "        # c,h,w = x.shape[1],x.shape[2],x.shape[3]\n",
    "        # t = self.Embedding * i\n",
    "        x = self.batchnorm(x)\n",
    "        dprint(f\"img.shape\",x.shape,debug)\n",
    "        latent = self.DownBlock(x)\n",
    "        dprint(f\"latent.shape\",latent.shape,debug)\n",
    "        latent = self.rearrange(latent)\n",
    "        \n",
    "        time_embedding = self.t(i)\n",
    "        latent = latent + time_embedding\n",
    "        \n",
    "        dprint(f\"latent.shape\",latent.shape,debug)\n",
    "        for i in range(TRANSFORMER_RECURSION):\n",
    "            latent = self.transformerencoder(latent)\n",
    "        mlp = self.mlp(latent)\n",
    "        # mlp2latent = self.mlp2_latent(mlp)\n",
    "        dprint(f\"mlp\",mlp.shape,debug)\n",
    "\n",
    "        # q = self.Embedding2 *mlp[0][0]\n",
    "        # self.Embedding3 = torch.matmul(self.Embedding3 ,mlp[:,1])\n",
    "        # self.Embedding4 = torch.matmul(self.Embedding4,mlp[:,2])\n",
    "        noise = mlp[:,1:]\n",
    "        noise = self.mlp2(noise)\n",
    "        dprint(\"noise.shape\",noise.shape,debug)\n",
    "        # t =  * i\n",
    "        # s = 0#self.Embedding * mlp\n",
    "        # tgt = latent + q + r + s\n",
    "        # tgt = F.normalize(tgt,dim=1)\n",
    "        # dprint(\"tgt.shape\",tgt.shape,debug)\n",
    "        # for i in range(TRANSFORMER_RECURSION):\n",
    "        #     tgt = self.transformerdecoder(latent,latent)\n",
    "        latent = self.rearrange2(latent)\n",
    "        noise = self.rearrange2(noise)\n",
    "        dprint(\"noise.shape\",noise.shape,debug)\n",
    "        dprint(\"latent.shape\",latent.shape,debug)\n",
    "        out_t = self.UpBlock(latent)\n",
    "        noise = self.UpBlock(noise)\n",
    "        dprint(\"out_t.shape\",out_t.shape,debug)\n",
    "        dprint(\"noise.shape\",noise.shape,debug)\n",
    "        # noise = out[:,C1:,:,:]\n",
    "        # out = out[:,:C1,:,:]\n",
    "        # dprint(f\"out.shape\",out.shape,debug)\n",
    "        # x = v2.functional.resize(x,size=(int(H*SCALE_FACTOR**mlp[0].item()),int(W*SCALE_FACTOR**mlp[0][0].item())))\n",
    "        # de_affine = mt.Affine(size=(H,W),degrees=-mlp[3].item(),translate1=0,translate2=0,scale=1-mlp[6].item())(x)\n",
    "        out = out_t.clone()\n",
    "        for i in range(x.shape[0]):\n",
    "            affine = mt.Affine(size=(H,W),scale=math.pow(SCALE_FACTOR,mlp[i][0].item()))(out_t[i])\n",
    "            out[i] = affine + noise[i]\n",
    "        # out = F.\n",
    "        # out = mlp[0][1] * x + (1-mlp[0][1]) * out\n",
    "        return out_t,mlp,out,noise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # img = read_image(TEST_IMG).float().to(\"cuda:0\", non_blocking=True)\n",
    "    # img = img.unsqueeze(0)\n",
    "    # img = mt.Resize(size=(H,W))(img)\n",
    "\n",
    "    model = AffinDiffusion().to(DEVICE, non_blocking=True)\n",
    "    # out = model(img,100)\n",
    "    # del img,out,model\n",
    "    # torch.cuda.empty_cache()\n",
    "    from torchinfo import summary\n",
    "    summary = summary(model,input_size=(2,C,H,W),col_names=[\"output_size\", \"num_params\"],row_settings=[\"var_names\"])\n",
    "    print(summary)\n",
    "    del model,summary\n",
    "    # print(torch.cuda.memory_summary())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchmetrics.image import MultiScaleStructuralSimilarityIndexMeasure\n",
    "from torchmetrics.image import PeakSignalNoiseRatio\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as tF\n",
    "import cv2\n",
    "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
    "\n",
    "\n",
    "def show(out,img):\n",
    "    with torch.no_grad():\n",
    "        pred = out[0,:,:,:]\n",
    "        orig = img[0,:,:,:]\n",
    "        imgs = torchvision.utils.make_grid([pred,orig],nrow=2)\n",
    "    if not isinstance(imgs, list):\n",
    "        imgs = [imgs]\n",
    "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
    "    for i, img in enumerate(imgs):\n",
    "        img = img.detach()\n",
    "        img = tF.to_pil_image(img)\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "    plt.show()\n",
    "\n",
    "def show_cv2(out,img):\n",
    "    cv2.imshow(\"out\",out)\n",
    "    cv2.imshow(\"img\",img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def label_loss(label,mlp):\n",
    "    label = torch.tensor(label,dtype=torch.float32)\n",
    "    mlp = mlp.view(5)\n",
    "    return torch.norm(label - mlp, dim=0)\n",
    "\n",
    "def diversity_loss(model):\n",
    "    return sum([i.Block[0].diversity_loss() for i in model.DownBlock])\n",
    "\n",
    "class Zero255Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Zero255Loss, self).__init__()\n",
    "    def forward(self, generated_image):\n",
    "        # 生成画像のテンソルを取得\n",
    "        image_tensor = generated_image.detach().cpu().numpy()\n",
    "        # 総ピクセル数を計算\n",
    "        total_pixels = image_tensor.size\n",
    "        # 値が0のピクセル数をカウント\n",
    "        zero_pixel_count = np.sum(image_tensor <= 0)\n",
    "        # 値が255のピクセル数をカウント\n",
    "        max_pixel_count = np.sum(image_tensor >= 255)\n",
    "        # 0と255のピクセルの割合を計算\n",
    "        ratio = (zero_pixel_count + max_pixel_count) / total_pixels\n",
    "        # 損失として割合を返す\n",
    "        loss = torch.tensor(ratio, dtype=torch.float32)\n",
    "        return loss\n",
    "\n",
    "def fid(img,out):\n",
    "    fid = FrechetInceptionDistance(feature=64)\n",
    "    img = img.to(torch.int8)\n",
    "    out = out.detach().to(torch.int8)\n",
    "    fid.update(img, real=True)\n",
    "    fid.update(out, real=False)\n",
    "    return fid.compute()\n",
    "\n",
    "def ms_ssim(img,out):\n",
    "    ms_ssim = MultiScaleStructuralSimilarityIndexMeasure(data_range=1.0)\n",
    "    return ms_ssim(img, out)\n",
    "\n",
    "def label_criterion(label,mlp):\n",
    "    label = torch.tensor(label,dtype=torch.float32)\n",
    "    mlp = mlp.view(DENOISE_PARAMS)\n",
    "    return torch.norm(label - mlp, dim=0)\n",
    "\n",
    "\n",
    "def save_weights(model,path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "def load_weights(model,path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"sasha/dog-food\")\n",
    "ds = ds.with_format(\"torch\", device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug=False\n",
    "image = read_image(TEST_IMG)\n",
    "# image = mt.Resize(size=(H,W))(image)\n",
    "image = image/255\n",
    "image = image.unsqueeze(0)\n",
    "*_,h,w = image.shape\n",
    "\n",
    "model = AffinDiffusion().to(DEVICE)\n",
    "model = load_weights(model,\"weights.pth\")\n",
    "\n",
    "zero255_loss = Zero255Loss()\n",
    "ergas = ErrorRelativeGlobalDimensionlessSynthesis()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "for batch in range(10000):\n",
    "    for data in ds[\"train\"]:\n",
    "        image = data[\"image\"]\n",
    "        c,h,w = image.shape\n",
    "        print(c,h,w)\n",
    "        # image = read_image(image)\n",
    "        image = image/255\n",
    "        image = image.unsqueeze(0)\n",
    "        if image.shape[2] < H or image.shape[3] < W:\n",
    "            continue\n",
    "    \n",
    "        if w == W:\n",
    "            x = 0\n",
    "            y = 0\n",
    "            xy = torch.zeros(2)\n",
    "        else:\n",
    "            x = np.random.randint(0,w-W)\n",
    "            y = np.random.randint(0,h-H)\n",
    "            xy = torch.tensor([x/(w-W),y/(h-H)]).float()\n",
    "        img = mt.Crop(size=(H,W),point=(y,x))(image).float()\n",
    "        noise_list = mt.generate_noise(num=DENOISE_TIME,noise_scale=NOISE_SCALE)\n",
    "        mx = my = np.linspace(2,H,DENOISE_TIME).astype(np.int32)\n",
    "        \n",
    "\n",
    "        for i in reversed(range(0,DENOISE_TIME-1)):\n",
    "            optimizer.zero_grad()\n",
    "            noisy_img,label = mt.noisy_image_generator(img,noise_list,i)\n",
    "            noisy_img_t,label_t = mt.noisy_image_generator(img,noise_list,i-1)\n",
    "        \n",
    "            label_t = label_t.unsqueeze(0)\n",
    "            noisy_img = noisy_img.to(DEVICE)\n",
    "            # label = label\n",
    "            noisy_img_t = noisy_img_t.to(DEVICE)\n",
    "            # label_t = label_t\n",
    "            \n",
    "            out_t,mlp,out, noise = model(noisy_img,torch.tensor([i],device=DEVICE))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # sigma = mlp[0][2].item()-label_t[0][2].item()\n",
    "            # if sigma < 0:\n",
    "            #     sigma = 0\n",
    "            # affine_out = mt.Affine(size=(H,W),scale=np.power(SCALE_FACTOR,mlp[0][0].item()))(out)\n",
    "            # pred_noise = mt.GaussianNoise(mean=mlp[0][1].item(),sigma=sigma)(out)\n",
    "            # pred = affine_out #- pred_noise\n",
    "            out_t = out_t.cpu()\n",
    "            out = out.cpu()\n",
    "            mlp = mlp.cpu()\n",
    "            noise = noise.cpu()\n",
    "            noisy_img = noisy_img.cpu()\n",
    "            noisy_img_t = noisy_img_t.cpu()\n",
    "            label_t = label_t.cpu()\n",
    "            label = label.cpu()\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "            # pred_xy = pred_xy.squeeze(0).cpu()\n",
    "            # pred_xy_ind = pred_xy * H\n",
    "            # pred_xy_ind = pred_xy_ind.int()\n",
    "            xy_loss = 0#criterion(xy,pred_xy)\n",
    "            label_loss = criterion(label_t.mean(dim=0),mlp.mean(dim=0))\n",
    "            # pixel_loss = zero255_loss(out_t)\n",
    "            weight_loss = diversity_loss(model)\n",
    "            ergas_loss = ergas(out_t, noisy_img_t)\n",
    "            mse_loss = criterion(out_t, noisy_img_t)\n",
    "            noise_mse_loss = criterion(out,noisy_img)\n",
    "            loss_of_loss = (mse_loss-noise_mse_loss)**2\n",
    "            loss_mean = criterion(noise.mean(),mlp[:,1])\n",
    "            loss_var = criterion(noise.var(),mlp[:,2])\n",
    "            # ms_ssim_loss = 1-ms_ssim(out_t, noisy_img_t)\n",
    "            # icecream.ic(batch,i,label_loss,mse_loss,noise_mse_loss,loss_of_loss,loss_mean,loss_var,ergas_loss,weight_loss)\n",
    "            loss =  label_loss + mse_loss + noise_mse_loss + loss_of_loss + loss_mean + loss_var + ergas_loss + weight_loss  #pixel_loss #+ ms_ssim_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if i % (DENOISE_TIME//10) == 0 :\n",
    "                icecream.ic(label_t,mlp,noise.mean(),noise.var())\n",
    "                icecream.ic(batch,i,label_loss,mse_loss,noise_mse_loss,loss_of_loss,loss_mean,loss_var,ergas_loss,weight_loss)\n",
    "                # outとnoisy_imgを結合\n",
    "                show(out_t,noisy_img_t)\n",
    "\n",
    "    \n",
    "        \n",
    "        grid_images = torch.cat([out_t,noisy_img_t], dim=0)\n",
    "        grid = make_grid(grid_images, nrow=2, normalize=True, padding=2)\n",
    "        save_image(grid, RESULT_DIR / f\"grid_image_batch_{batch}.png\")\n",
    "\n",
    "        save_weights(model,\"weights.pth\")   \n",
    "\n",
    "        if batch % 1== 0:\n",
    "            with torch.no_grad():\n",
    "                noisy_image,label = mt.noisy_image_generator(img,noise_list,DENOISE_TIME-1)\n",
    "                noisy_image = noisy_image.to(DEVICE)\n",
    "                out = noisy_image.clone().to(DEVICE)\n",
    "                # noisy_img.requires_grad = True\n",
    "                label = label.to(DEVICE)\n",
    "                for i in reversed(range(1,DENOISE_TIME)):\n",
    "                    out,_ ,_,_= model(out,torch.tensor([i],device=DEVICE))\n",
    "                    if i % (DENOISE_TIME) == 0 :\n",
    "                        grid_images = torch.cat([out.detach().cpu(),noisy_image.detach().cpu()],dim=0)\n",
    "                        grid = make_grid(grid_images, nrow=2, normalize=True, padding=3)\n",
    "                        # save_image(grid, RESULT_DIR / f\"grid_image_batch_{batch}.png\")\n",
    "                        img = tF.to_pil_image(grid)\n",
    "                        fig, axs = plt.subplots(ncols=1, squeeze=False)\n",
    "                        axs[0, 0].imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AffinDiffusion().to(DEVICE)\n",
    "model = load_weights(model,\"weights.pth\")\n",
    "\n",
    "model.eval()\n",
    "img = torch.randn(1,3,H,W)\n",
    "img = img.to(DEVICE)\n",
    "for i in range(DENOISE_TIME):\n",
    "    img,_,_ = model(img)\n",
    "    if i % 10 == 0:\n",
    "        show(out,img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# CUDAが利用可能かどうかを確認\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(f\"CUDA利用可能: {cuda_available}\")\n",
    "\n",
    "if cuda_available:\n",
    "    # 利用可能なGPUデバイスの数を表示\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print(f\"利用可能なGPUデバイス数: {gpu_count}\")\n",
    "    \n",
    "    # 現在のGPUデバイスの名前を表示\n",
    "    current_device = torch.cuda.current_device()\n",
    "    device_name = torch.cuda.get_device_name(current_device)\n",
    "    print(f\"現在のGPUデバイス: {device_name}\")\n",
    "else:\n",
    "    print(\"CUDAが利用できません。CPUで実行されます。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
